name	ring	quadrant	isNew	status	description
Path-to-production mapping	Adopt	Techniques	TRUE	No Change	<p>Although <strong>path-to-production mapping</strong> has been a near-universal practice at Thoughtworks since codifying <em><a href="https://www.amazon.com/Continuous-Delivery-Deployment-Automation-Addison-Wesley/dp/0321601912">Continuous Delivery</a></em>, we often come across organizations unfamiliar with the practice. The activity is most often done in a workshop with a cross-functional group of people —  that includes everyone involved in designing, developing, releasing and operating the software — around a shared whiteboard (or virtual equivalent). First, the steps in the process are listed in order, from the developer workstation all the way to production. Then, a facilitated session is used to capture further information and pain points. The most common technique we see is based on <a href="https://en.wikipedia.org/wiki/Value-stream_mapping">value-stream mapping</a>, although plenty of <a href="https://caroli.org/en/path-to-production/">process map</a> variants are equally valuable. The activity is often eye-opening for many of the participants, as they identify delays, risks and inconsistencies and continue to use the visual representation for the continuous improvement of the build and deploy process. We consider this technique so foundational that we were surprised to discover we hadn't blipped it before.</p>
Team cognitive load	Adopt	Techniques	FALSE	No Change	<p>Team interaction is a key concept when redesigning an organization for business agility and speed. These interactions will be reflected in the software being built (see <a href="https://www.thoughtworks.com/about-us/news/2021/latest-thoughtworks-technology-radar-proclaims---embrace-conway-">Conway's Law</a>) and indicate how effectively teams can autonomously deliver value to their customers. Our advice is to be intentional about how teams are designed and how they interact. Because we believe that organizational design and team interactions evolve over time, we think it's particularly important to measure and keep track of the <strong>team cognitive load</strong>, which indicates how easy or difficult teams find building, testing and maintaining their services. We've been using a <a href="https://github.com/TeamTopologies/Team-Cognitive-Load-Assessment">template</a> to assess team cognitive load that is based on ideas by the authors of the <em><a href="https://teamtopologies.com/book">Team Topologies</a></em> book.</p>  <p>We continue to be impressed by the positive impact of applying this book's concepts when communicating to clients and redesigning organizations. The authors recommend a simple but powerful approach to organizational design, identifying just four types of teams and three modes of interaction; this helps reduce ambiguity within the organization and provides a common vocabulary for teams, stakeholders and leadership to describe and design a team's work. To implement an org design change, we design the ideal to-be team topologies structure, apply any technical/staffing constraints (i.e., not enough employees) and then end up with the final to-be structure. That allows us to better advise clients and anticipate whether we're indeed improving cognitive load by comparing the as-is/to-be team structures.</p>
Threat modeling	Adopt	Techniques	FALSE	No Change	<p>We continue to recommend that teams carry out <strong><a href="https://www.owasp.org/index.php/Category:Threat_Modeling">threat modeling</a></strong> — a set of techniques to help you identify and classify potential threats during the development process — but we want to emphasize that this is not a one-off activity only done at the start of projects; teams need to avoid the <a href="/radar/techniques/security-sandwich">security sandwich</a>. This is because throughout the lifetime of any software, new threats will emerge and existing ones will continue to evolve thanks to external events and ongoing changes to requirements and architecture. This means that threat modeling needs to be repeated periodically — the frequency of repetition will depend on the circumstances and will need to consider factors such as the cost of running the exercise and the potential risk to the business. When used in conjunction with other techniques, such as establishing cross-functional security requirements to address common risks in the project's technologies and using automated security scanners, threat modeling can be a powerful asset.</p>
Incremental developer platform	Trial	Techniques	TRUE	No Change	<p>We've been writing about developer platforms and how to build them in almost every edition of the Radar since 2017. In the meantime, the <em><a href="https://teamtopologies.com/book">Team Topologies</a></em> book has also done a great job of describing the ideal of a platform that supports developers with "self-service APIs, tools, services and knowledge." However, we often see teams shooting for too much of that platform vision too fast. Instead, building an <strong>incremental developer platform</strong> is key.</p>  <p><em>Team Topologies</em> recommends to always strive for what they call the "Thinnest Viable Platform" necessary at any given stage, where the first version could even be just a set of documentation on a wiki. The next increment could increase the service level by providing templates or allowing teams to create pull requests. Further increments could then introduce self-service APIs, but only if valuable. In short, even though we've cautioned against fully <a href="/radar/techniques/ticket-driven-platform-operating-models">ticket-driven platform operating models</a>, going from zero to self-service is the other extreme. Pace yourself, <a href="/radar/techniques/applying-product-management-to-internal-platforms">treat your platform as a product</a> and build it up incrementally.</p>
Observability for CI/CD pipelines	Trial	Techniques	TRUE	No Change	<p>Observability practices have shifted the conversation from monitoring for well-understood problems to helping troubleshoot unknown problems in distributed systems. We've seen success taking that perspective outside of the traditional production environment by applying <strong>observability for CI/CD pipelines</strong> to help optimize testing and deployment bottlenecks. Complex pipelines create developer friction when they run too slow or suffer from nondeterminism, reducing important feedback loops and hindering developer effectiveness. Additionally, their role as critical deployment infrastructure creates stress points during periods of rapid deployments, as happened to several organizations responding to the recent log4shell vulnerability. The concept of traces translates nicely to pipelines: instead of capturing the cascade of service calls, child spans capture information about each stage of the build. The same waterfall charts used to analyze a call flow in a distributed architecture can also be effective in helping us to identify bottlenecks in pipelines, even complex ones with fan-in and fan-out. This enables far more focused optimization efforts. While the technique should work with any tracing tool, <a href="https://www.honeycomb.io/">Honeycomb</a> supports a tool called <a href="https://github.com/honeycombio/buildevents">buildevents</a> that helps capture pipeline trace information. An alternative approach of capturing information already exposed by CI/CD platforms, taken by the open-source <a href="https://github.com/cburgmer/buildviz">buildviz</a> (built and maintained by a Thoughtworker), allows for a similar investigation without changing the step configurations themselves.</p>
SLSA	Trial	Techniques	FALSE	No Change	<p>As software continues to grow in complexity, the threat vector of software dependencies becomes increasingly challenging to guard against. Supply chain Levels for Software Artifacts, or <strong><a href="https://slsa.dev/">SLSA</a></strong> (pronounced "salsa"), is a consortium-curated set of guidance for organizations to protect against supply chain attacks, evolved from internal guidance Google has been using for years. We appreciate that SLSA doesn't promise a "silver bullet," tools-only approach to securing the supply chain, but it does provide a checklist of concrete threats and practices along a maturity model. The <a href="https://slsa.dev/spec/v0.1/threats">threat model</a> is easy to follow with real-world examples of attacks, and the <a href="https://slsa.dev/spec/v0.1/requirements">requirements</a> provide guidance to help organizations prioritize actions based on levels of increasing robustness to improve their supply chain security posture. Since we first mentioned it in the Radar, SLSA has added more detail around <a href="https://slsa.dev/attestation-model">software attestations</a> with examples to track concerns like <a href="https://slsa.dev/provenance/v0.2">build provenance</a>. Our teams have found SLSA to strike a nice balance between implementation guidance and higher-level awareness around supply chain threats.</p>
Software Bill of Materials	Trial	Techniques	FALSE	No Change	<p>With continued pressure to keep systems secure and no reduction in the general threat landscape, a machine-readable <strong>Software Bill of Materials</strong> (SBOM) may help teams stay on top of security problems in the libraries that they rely on. Since the original <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/">Executive Order</a> was published, the industry has gained clarity and understanding of what an SBOM is and how to create one; the National Institute of Standards and Technology (NIST), for example, now has more <a href="https://www.nist.gov/itl/executive-order-14028-improving-nations-cybersecurity/software-security-supply-chains-software-1">specific advice</a> on how to comply with the order. We've had production experience using SBOMs on projects ranging from small companies to large multinationals and even government departments, and we're convinced they provide a benefit. More organizations and governments should consider requiring SBOMs for the software they use. The technique will be strengthened by the new tools that continue to emerge, such as the <a href="https://firebase.google.com/docs/android/learn-more#bom">Firebase Android BOM</a> that automatically aligns an application's library dependencies to those listed in the BOM.</p>
Carbon efficiency as an architectural characteristic	Assess	Techniques	TRUE	No Change	<p>Sustainability is a topic that demands the attention of enterprises. In the software development space its importance has increased, and we're now seeing <a href="https://www.thoughtworks.com/clients/Bringing-green-cloud-optimization-to-a-green-energy-business">different ways</a> to approach this topic. Looking at the carbon footprint of building software, for example, we recommend assessing <strong>carbon efficiency as an architectural characteristic</strong>. An architecture that takes into consideration carbon efficiency is one where design and infrastructure choices have been made in order to to minimize energy consumption and therefore carbon emissions. The measurement tooling and advice in this space is maturing, making it feasible for teams to consider carbon efficiency alongside other factors such as performance, scalability, financial cost and security. Like almost everything in software architecture, this should be considered a trade-off; our advice is to think about this as one additional characteristic in a whole set of relevant <a href="https://en.wikipedia.org/wiki/List_of_system_quality_attributes">quality attributes</a> that are driven and prioritized by organizational goals and not left to a small cadre of experts to ponder in a siloed manner.</p>
GitHub push protection	Assess	Techniques	TRUE	No Change	<p>The accidental publication of secrets seems to be a perennial issue with tools such as <a href="/radar/tools/talisman">Talisman</a> popping up to help with the problem. Before now, GitHub Enterprise Cloud users with an Advanced Security License could enable security scanning on their accounts, and any secrets (API keys, access tokens, credentials, etc.) that were accidentally committed and pushed would trigger an alert. <strong><a href="https://docs.github.com/en/enterprise-cloud@latest/code-security/secret-scanning/protecting-pushes-with-secret-scanning">GitHub push protection</a></strong> takes this one step further, and brings it one step earlier in the development workflow, by blocking changes from being pushed at all if secrets are detected. This needs to be configured for the organization and applies, of course, only to license holders, but additional protection from publishing secrets is to be welcomed.</p>
Local-first application	Assess	Techniques	TRUE	No Change	<p>In a centralized application, the data on the server is the single source of truth — any modification to the data must go through the server. Local data is subordinate to the server version. This seems like a natural and inevitable choice to enable collaboration among multiple users of the software. <strong>Local-first application</strong>, or <a href="https://www.inkandswitch.com/local-first/#towards-a-better-future">local-first software</a>, is a set of principles that enables both collaboration and local data ownership. It prioritizes the use of local storage and local networks over servers in remote data centers or the cloud. Techniques like conflict-free replicated data types (CRDTs) and peer-to-peer (P2P) networks have the potential to be a foundational technology for realizing local-first software.</p>
SLIs and SLOs as code	Assess	Techniques	TRUE	No Change	<p>Since Google first popularized service-level indicators (SLIs) and service-level objectives (SLOs) as part of their site reliability engineering (SRE) practice, observability tools like <a href="https://docs.datadoghq.com/monitors/service_level_objectives/">Datadog</a>, <a href="https://www.honeycomb.io/slos">Honeycomb</a> and <a href="https://www.dynatrace.com/news/blog/what-are-slos/">Dynatrace</a> started incorporating SLO monitoring into their toolchains. <a href="https://github.com/OpenSLO/OpenSLO">OpenSLO</a> is an emerging standard that allows defining <strong>SLIs and SLOs as code</strong>, using a declarative, vendor-neutral specification language based on the YAML format used by <a href="/radar/platforms/kubernetes">Kubernetes</a>. While the standard is still quite new, we're seeing some encouraging momentum, as with Sumo Logic's contribution of the <a href="https://github.com/OpenSLO/slogen">slogen</a> tool to generate monitoring and dashboards. We're excited by the promise of versioning SLI and SLO definitions in code and updating observability tooling as part of the CI/CD pipeline of the service being deployed.</p>
Superficial cloud native	Hold	Techniques	TRUE	No Change	<p>The term "cloud native" was originally used to describe architectures with characteristics that took maximum advantage of public cloud hosting. Examples include distributed architectures composed of many small, stateless and collaborating processes, and systems with high levels of automation for building, testing and deploying applications. However, we've noticed a growing trend toward <strong>superficial cloud native</strong> designs that simply use a lot of a cloud vendor's proprietary services and stop there without revisiting the fundamentally monolithic, brittle or toil-intensive nature of the application. It’s important to remember that serverless functions by themselves don't make an application more resilient or easier to maintain and that cloud native is really a matter of design rather than a set of implementation choices.</p>
Backstage	Adopt	Platforms	FALSE	No Change	<p>In an increasingly digital world, improving developer effectiveness in large organizations is often a core concern of senior leaders. We've seen enough value with developer portals in general and <strong><a href="https://backstage.io/">Backstage</a></strong> in particular that we're happy to recommend it in Adopt. Backstage is an open-source developer portal platform created by Spotify that improves discovery of software assets across the organization. It uses Markdown <a href="https://backstage.io/docs/features/techdocs/techdocs-overview">TechDocs</a> that live alongside the code for each service, which nicely balances the needs of centralized discovery with the need for distributed ownership of assets. Backstage supports software templates to accelerate new development and a plugin architecture that allows for extensibility and adaptability into an organization's infrastructure ecosystem. <a href="https://backstage.io/docs/features/software-catalog/software-catalog-overview">Backstage Service Catalog</a> uses YAML files to track ownership and metadata for all the software in an organization's ecosystem; it even lets you track third-party SaaS software, which usually requires tracking ownership.</p>
AWS Database Migration Service	Trial	Platforms	TRUE	No Change	<p>Many of our teams have successfully used <strong><a href="https://aws.amazon.com/dms/">AWS Database Migration Service</a></strong> (DMS) to migrate data to and from AWS. In one of our Digital Transformation engagements, we achieved nearly zero downtime cut-over to the new system as we migrated data from Microsoft SQL Server to an AWS Relational Database Service (RDS) PostgreSQL instance. Such transformations involve many moving parts that require planning and coordination across multidisciplinary teams, but for data migration we're quite happy with DMS. It automatically manages the deployment, management and monitoring of all required resources. Over the years DMS has matured to support several <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.html">source</a> and <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html">target</a> databases, and we continue to like it.</p>
Colima	Trial	Platforms	FALSE	No Change	<p><strong><a href="https://github.com/abiosoft/colima">Colima</a></strong> is becoming a popular open alternative to Docker Desktop. It provisions the <a href="/radar/platforms/docker">Docker</a> container run time in a Lima VM, configures the Docker CLI on macOS and handles port-forwarding and volume mounts. Colima uses <a href="https://containerd.io/">containerd</a> as its run time, which is also the run time on most managed <a href="/radar/platforms/kubernetes">Kubernetes</a> services — improving the important dev-prod parity. With Colima you can easily use and test the latest features of containerd, such as lazy loading for container images. We've been having good results with Colima in our projects. When in the Kubernetes space, we also use <a href="https://github.com/containerd/nerdctl">nerdctl</a>, a Docker-compatible CLI for containerd. Since Kubernetes has deprecated Docker as container run time and most managed-services (EKS, GKE, etc) are following its lead, more people will be looking to containerd native tools, hence the importance of tools like nerdctl. In our opinion, Colima is realizing its strong potential and becoming a go-to option as an alternative to Docker Desktop.</p>
eBPF	Trial	Platforms	refresh_writeup	No Change	<p>For several years now, the Linux kernel has included the extended Berkeley Packet Filter (<strong><a href="https://ebpf.io/">eBPF</a></strong>), a virtual machine that provides the ability to attach filters to particular sockets. But eBPF goes far beyond packet filtering and allows custom scripts to be triggered at various points within the kernel with very little overhead. By allowing you to run sandboxed programs within the operating system kernel, application developers can run eBPF programs to add additional capabilities to the operating system at run time. Some of our projects require troubleshooting and profiling at the system call level, and our teams found that tools like <a href="https://github.com/iovisor/bcc">bcc</a> and <a href="https://github.com/iovisor/bpftrace">bpftrace</a> have made their jobs easier. Observability and network infrastructure also benefit from eBPF — for example, the <a href="/radar/tools/cilium">Cilium</a> project can implement traffic load balancing and observability <a href="/radar/techniques/service-mesh-without-sidecar">without sidecar overhead</a> in <a href="/radar/platforms/kubernetes">Kubernetes</a>, and <a href="https://github.com/cilium/hubble">Hubble</a> provides further security and traffic observability on top of it. The <a href="https://github.com/falcosecurity/falco">Falco</a> project uses eBPF for security monitoring, and the <a href="https://github.com/facebookincubator/katran">Katran</a> project uses eBPF to build more efficient L4 load balancing. The eBPF community is growing rapidly, and we're seeing more and more synergy with the field of observability.</p>
Teleport	Trial	Platforms	FALSE	No Change	<p><strong><a href="https://gravitational.com/teleport/">Teleport</a></strong> is a tool for <a href="/radar/techniques/zero-trust-architecture">zero trust</a> network access to infrastructure. Traditional setups require complex policies or jump servers to restrict access to critical resources. Teleport, however, simplifies this with a unified access plane and with fine-grained authorization controls that replace jump servers, VPNs or shared credentials. Implemented as a single binary with out-of-the-box support for several protocols (including SSH, RDP, <a href="/radar/platforms/kubernetes">Kubernetes</a> API, MySQL, <a href="/radar/platforms/mongodb">MongoDB</a> and PostgreSQL wire protocols), Teleport makes it easy to set up and manage secured access across Linux, Windows or Kubernetes environments. Since we first mentioned it in the Radar, a few teams have used Teleport and our overall positive experience prompted us to highlight it.</p>
IAM Roles Anywhere	Assess	Platforms	TRUE	No Change	<p><strong><a href="https://docs.aws.amazon.com/rolesanywhere/latest/userguide/introduction.html">IAM Roles Anywhere</a></strong> is a new service from AWS that lets you obtain temporary security credentials in IAM for workloads such as servers, containers and applications that run outside of AWS. We find it particularly useful in hybrid cloud setups where workloads are split across AWS and non-AWS resources. Instead of creating long-lived credentials, with IAM Roles Anywhere, you can now create short-lived credentials to access AWS resources using X.509 certificates. We believe this approach streamlines the access pattern across the hybrid cloud and recommend you check it out.</p>
Keptn	Assess	Platforms	TRUE	No Change	<p><strong><a href="https://keptn.sh/">Keptn</a></strong> is a control plane for delivery and operations that relies on <a href="https://cloudevents.io/">CloudEvents</a> for instrumentation. Like one of the techniques we mentioned in <a href="/radar/techniques/observability-for-ci-cd-pipelines">observability for CI/CD pipelines</a>, Keptn visualizes its orchestration as traces. The declarative definition of the delivery pipeline aims to separate SRE intentions from the underlying implementation, relying on other observability, pipeline and deployment tooling to respond to the appropriate events. We're particularly excited by the idea of adding service-level objective (SLO) verifications as <a href="/radar/techniques/architectural-fitness-function">architectural fitness functions</a> to CI/CD pipelines: Keptn lets you define service-level indicators (SLIs) as key-value pairs, with the value representing the query to your observability infrastructure. It will then evaluate the result against the defined SLOs as a <a href="https://keptn.sh/docs/concepts/quality_gates/">quality gate</a>. Keptn takes the same approach to automated operations, allowing a declarative definition that specifies the intent of scaling a ReplicaSet in response to a degradation of average response time, for example. Created by Dynatrace, Keptn also integrates with <a href="/radar/tools/prometheus">Prometheus</a> and Datadog.</p>
Great Expectations	Adopt	Tools	FALSE	No Change	<p><a href="https://docs.greatexpectations.io/en/latest/"><strong>Great Expectations</strong></a> has become a sensible default for our teams in the data quality space, which is why we recommend adopting it — not only for the lack of better alternatives but also because our teams have reported great results in several client projects. Great Expectations is a framework that allows you to craft built-in controls that flag anomalies or quality issues in data pipelines. Just as unit tests run in a build pipeline, Great Expectations makes assertions during the execution of a data pipeline. We like its simplicity and ease of use — the rules stored in JSON can be modified by our data domain experts without necessarily needing data engineering skills.</p>
AWS Backup Vault Lock	Trial	Tools	TRUE	No Change	<p>When implementing robust, secure and reliable disaster recovery, it’s necessary to ensure that backups can't be deleted or altered before their expiry, either maliciously or accidentally. Previously, with AWS Backup, these policies and guarantees had to be implemented by hand. Recently, AWS has added the Vault Lock feature to ensure backups are immutable and untamperable. <a href="https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html"><strong>AWS Backup Vault Lock</strong></a> enforces retention and deletion policies and prevents even those with administrator privileges from altering or deleting backup files. This has proved to be a valuable addition and fills a previously empty space.</p>
AWS Control Tower	Trial	Tools	TRUE	No Change	<p>Multi-team account management is a challenge in AWS, especially in setup and governance; <a href="https://aws.amazon.com/controltower"><strong>AWS Control Tower</strong></a> is an attempt to address this challenge. Our team has reported good results using it to manage accounts and access control for multiple teams in the organization through a single, centralized place.</p>
Clumio Protect	Trial	Tools	TRUE	No Change	<p>We've had success with <a href="https://clumio.com/products/protect/"><strong>Clumio Protect</strong></a> for backing up AWS data, particularly S3. A commercial SaaS solution, Clumio Protect can also back up a range of other AWS services and stores the data offline where it is not accessible through the internet. Our teams responsible for handling data protection and recovery at massive scale found that Clumio Protect is easy to set up and maintain and far outperforms the native AWS Backup service when S3 buckets are particularly big.</p>
Cruft	Trial	Tools	TRUE	No Change	<p>We've been talking about <a href="/radar/techniques/tailored-service-templates">tailored service templates</a> ever since we first identified <a href="/radar/techniques/microservices">microservices</a> as a thing. If an organization sets out to create a collection of small services that can be developed, built, deployed and operated independently but consistently, it makes sense to give teams a solid starting point that aligns to the standard. However, one of the enduring problems with that approach is that as the template evolves over time in response to changing technical and business requirements, projects based on older versions of the template fall out of date. Retrofitting template improvements into an established project becomes a major pain. <strong><a href="https://cruft.github.io/cruft/">Cruft</a></strong> attempts to address this problem by providing tools to identify and patch differences between a local project and the current head of a master template repository. It combines the <a href="https://github.com/cookiecutter/cookiecutter">Cookiecutter</a> templating engine with git hashes to identify and apply changes to the templates. Think of it as a package manager for a project boilerplate. Keeping templates up-to-date is a notoriously difficult and long-standing problem, so to us the solution Cruft provides sounds almost too good to be true. Based on early feedback from our team, however, Cruft actually works and makes life easier for service builders and maintainers. We're anxious to see how it performs over the long term, but for now it's worth taking a look at this potentially useful tool.</p>
Hadolint	Trial	Tools	TRUE	No Change	<p>We like spreading the word about linting tools that actually help you find issues rather than just shortcut style disputes in the team. <strong><a href="https://github.com/hadolint/hadolint">Hadolint</a></strong> is one of those tools — it helps find common issues in Dockerfiles. We find it to be fast, accurate and with good documentation. It explains both how to fix an issue and why it's an issue in the first place, thus nudging Dockerfile authors toward good practices. Incidentally, Hadolint is built on top of <a href="/radar/tools/shellcheck">ShellCheck</a>, which we recommend in its own right for checking your shell scripts.</p>
Kaniko	Trial	Tools	TRUE	No Change	<p>Most of today's CI/CD pipeline tools and platforms are built on containers as runtimes. Many of our teams are using <strong><a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a></strong> to build container images from within those container-based pipelines. This comes as part of a trend away from <a href="/radar/platforms/docker">Docker</a> as the de facto standard for container runtimes. With Kaniko, you can build your images without using a Docker daemon. This helps avoid the security issue of Docker's "privileged" mode, which would be necessary for any "Docker-in-Docker" activity. Moreover, you don't have to assume that your pipeline has access to a Docker daemon in the first place, which cannot be taken for granted anymore and often requires extra configuration.</p>
Clasp	Assess	Tools	TRUE	No Change	<p>Unfortunately, a big part of the world still runs on spreadsheets and will continue to do so. They're the ultimate tool to let anyone build those small custom tools tailored to their exact needs. However, when you want to enhance them with a level of logic that requires "real" code, the low-code nature of spreadsheets can then become a constraint. If you're with a company that, like Thoughtworks, uses Google's G-Suite, <strong><a href="https://github.com/google/clasp">Clasp</a></strong> enables you to apply at least some <a href="/radar/techniques/continuous-delivery-cd">Continuous Delivery</a> practices to Apps Script code. You can write the code outside of the Apps Script project, which creates options for testing, source control and build pipelines; it even lets you use <a href="/radar/languages-and-frameworks/typescript">TypeScript</a>. Clasp has been around for a while, and you shouldn’t expect a programming environment with all of the usual comforts, but it can greatly improve the experience of using Apps Script.</p>
git-together	Assess	Tools	TRUE	No Change	<p>We're always looking for ways to remove small frictions from pair programming, which is why we're excited by <a href="https://github.com/kejadlen/git-together"><strong>git-together</strong></a>, a tool written in Rust that simplifies git commit attribution during pairing. By aliasing <code>git-together</code> as <code>git</code>, the tool allows you to add extensions to <code>git config</code> that capture committer information, aliasing each committer by their initials. Changing pairs (or switching to soloing or mob programming) requires you to run <code>git with</code>, followed by the initials of the pair (for example: <code>git with bb cc</code>), allowing you to resume your regular git workflow afterward. Every time you commit, git-together will rotate through the pair as the official author that git stores, and it will automatically add any other authors to the bottom of the commit message. The configuration can be checked in with the repo, allowing git-together to work automatically after cloning a repo.</p>
Harness Cloud Cost Management	Assess	Tools	TRUE	No Change	<p><strong><a href="https://harness.io/products/cloud-cost">Harness Cloud Cost Management</a></strong> is a commercial tool that works across all three of the major cloud providers and their managed <a href="/radar/platforms/kubernetes">Kubernetes</a> clusters to help visualize and manage cloud costs. The product calculates a cost efficiency score by looking at idle resources as well as resources not allocated to any workload and uses historical trends to help optimize resource allocation. The dashboards highlight cost spikes and allow a user to register unexpected anomalies, which are then fed into their reinforcement learning algorithm around anomaly detection. Cloud Cost Management can recommend adjustments to limits for memory and CPU usage, with options to optimize for either cost or performance. "Perspectives" allows you to group costs based on organizationally defined filters (which could correspond to business units, teams or products) and automate report distribution to bring visibility into cloud spend. We believe Cloud Cost Management offers a compelling feature set to help organizations mature their FinOps practices.</p>
Infracost	Assess	Tools	FALSE	No Change	<p>We continue to see organizations move to the cloud without properly understanding how they will track ongoing spend. We previously blipped <a href="/radar/techniques/run-cost-as-architecture-fitness-function">run cost as architecture fitness function</a>, and <a href="https://infracost.io/"><strong>Infracost</strong></a> is a tool that aims to make these cloud cost trade-offs visible in Terraform pull requests. It's open-source software and available for macOS, Linux, Windows and Docker and supports pricing for AWS, GCP and Microsoft Azure out of the box. It also provides a public API that can be queried for current cost data. We remain excited by its potential, especially when it comes to gaining better cost visibility in the IDE.</p>
Karpenter	Assess	Tools	TRUE	No Change	<p>One of the fundamental capabilities of <a href="/radar/platforms/kubernetes">Kubernetes</a> is its ability to automatically launch new pods when additional capacity is needed and shut them down when loads decrease. This horizontal autoscaling is a useful feature, but it can only work if the nodes needed to host the pods already exist. While <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">Cluster Autoscaler</a> can do some rudimentary cluster expansion triggered by pod failures, it has limited flexibility; <strong><a href="https://karpenter.sh/">Karpenter</a></strong>, however, is an open-source <a href="/radar/tools/kubernetes-operators">Kubernetes Operator</a> autoscaler with more smarts built in: it analyzes the current workloads and the pod scheduling constraints to automatically select an appropriate instance type and then start or stop it as needed. Karpenter is an operator in the spirit of tools like <a href="/radar/tools/crossplane">Crossplane</a> that can provision cloud resources outside the cluster. Karpenter is an attractive companion to the autoscaling services cloud vendors provide natively with their managed Kubernetes clusters. For example, AWS now supports Karpenter as a first-class alternative in their EKS Cluster Autoscaler service.</p>
Mizu	Assess	Tools	TRUE	No Change	<p><strong><a href="https://github.com/up9inc/mizu/tree/main">Mizu</a></strong> is an API traffic viewer for <a href="/radar/platforms/kubernetes">Kubernetes</a>. Unlike other tools, Mizu does not require instrumentation or code changes. It runs as a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> to inject a container at the node level in your Kubernetes cluster and performs tcpdump-like operations. We find it useful as a debugging tool, as it can observe all API communications across multiple protocols (REST, gRPC, <a href="/radar/platforms/apache-kafka">Kafka</a>, AMQP and <a href="/radar/platforms/redis">Redis</a>) in real time.</p>
Soda Core	Assess	Tools	TRUE	No Change	<p><a href="https://www.soda.io/core"><strong>Soda Core</strong></a> is an open-source data quality and observability tool. We talked about <a href="/radar/tools/great-expectations">Great Expectations</a> previously in the Radar, and Soda Core is an alternative with a key difference — you express the data validations in a DSL called <a href="https://docs.soda.io/soda-cl/soda-cl-overview.html">SodaCL</a> (previously called <a href="https://docs.soda.io/soda-sql/overview.html">Soda SQL</a>) as opposed to Python functions. Once the validations are written, it can be executed as part of a <a href="https://docs.soda.io/soda-core/orchestrate-scans.html">data pipeline</a> or <a href="https://docs.soda.io/soda-core/programmatic.html">scheduled to run programmatically</a>. As we become increasingly data-driven, it's critical to maintain data quality, and we encourage you to assess Soda Core.</p>
Teller	Assess	Tools	TRUE	No Change	<p><strong><a href="https://github.com/tellerops/teller">Teller</a></strong> is an open-source universal secret manager for developers that ensures the correct environment variables are set when starting an application. However, it's not a vault itself — it's a CLI tool that connects to a variety of sources, ranging from cloud secrets providers to third-party solutions like <a href="/radar/tools/hashicorp-vault">HashiCorp Vault</a> to local environment files. Teller has additional functionality to scan for vault-kept secrets in your code, to redact secrets from logs, to detect drift between secrets providers and to sync between them. Given the sensitivity of accessing secrets, we can't emphasize enough the need to secure the supply chain for open-source dependencies, but we appreciate how easy the CLI is to use in local development environments, CI/CD pipelines and deployment automation.</p>
Xcode Cloud	Assess	Tools	TRUE	Move Out	<p><a href="https://developer.apple.com/xcode-cloud/"><strong>Xcode Cloud</strong></a> is a CI/CD tool that is built into Xcode and used to build, test and deploy Apple apps. It provides an integrated experience with familiar tools for Apple developers like Xcode, App Store Connect and TestFlight. Based on our team's experience, it does a good job of simplifying the pipeline configuration and provisioning profiles and certificates. This tool is quite fresh and most of our mobile development teams are still using the more mature <a href="/radar/tools/bitrise">Bitrise</a>. Still, we think it's worth assessing and tracking its progress.</p>
Astro	Assess	languages-and-frameworks	TRUE	Move In	<p>It's hard to believe, but in 2022, the developer community continues to pump out interesting new frameworks for building web applications. <strong><a href="https://astro.build/">Astro</a></strong> is a recent, open-source, multi-page application framework that renders HTML on the server and minimizes the amount of JavaScript sent over the wire. Astro seems particularly well-suited to content-oriented websites that pull from many different sources. We like the fact that although Astro encourages sending only HTML, it still supports — when appropriate — select active components written in the front-end JavaScript framework of your choice. It does this through its <a href="https://mainawycliffe.dev/blog/island-architecture/">island architecture</a>. Islands are regions of interactivity within a single page where the necessary JavaScript is downloaded only when needed. Astro is relatively new but seems to support a growing ecosystem of developers and code. It's one to watch as it develops.</p>
Kristoffers two-person integrity Vault in Rust	Assess	Tools	TRUE	New	<p>How marvelous it would be, in 2024, to avoid having root credentials in a physical safe, having to be on-site if they are needed, without any proper audit registration. With this facinating system, two persons can SAML authenticate, and if they are member of the proper groups, get the TOTP for the MFA challenge when using the root user in a AWS account.
		Infrastructure			
		Datastores			
		Data management			